{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of Text Generation with Huginn-01/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Mikolaj/recurrent-pretraining/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# support running without installing as a package\n",
    "wd = Path.cwd().parent\n",
    "sys.path.append(str(wd))\n",
    "# import litgpt # noqa: F401\n",
    "\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer, GenerationConfig\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Message:\n",
    "    role: str\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5410, -0.2934, -2.1788])\n",
      "tensor([ 0.9941, -0.1893, -1.4056], grad_fn=<MulBackward0>)\n",
      "tensor([ 0.9941, -0.1893, -1.4056], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "rms = nn.RMSNorm(3, eps=0.0001)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "v = torch.randn(3)\n",
    "\n",
    "v2 = rms(v)\n",
    "v3 = rms(v2)\n",
    "\n",
    "print(v)\n",
    "print(v2)\n",
    "print(v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RavenForCausalLM(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(65536, 5280)\n",
       "    (prelude): ModuleList(\n",
       "      (0-1): 2 x SandwichBlock(\n",
       "        (norm_1): RMSNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (Wqkv): Linear(in_features=5280, out_features=15840, bias=False)\n",
       "          (proj): Linear(in_features=5280, out_features=5280, bias=False)\n",
       "        )\n",
       "        (norm_2): RMSNorm()\n",
       "        (mlp): GatedMLP(\n",
       "          (fc): Linear(in_features=5280, out_features=35840, bias=False)\n",
       "          (proj): Linear(in_features=17920, out_features=5280, bias=False)\n",
       "          (nonlin): SiLU()\n",
       "        )\n",
       "        (norm_3): RMSNorm()\n",
       "        (norm_4): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (adapter): Linear(in_features=10560, out_features=5280, bias=False)\n",
       "    (core_block): ModuleList(\n",
       "      (0-3): 4 x SandwichBlock(\n",
       "        (norm_1): RMSNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (Wqkv): Linear(in_features=5280, out_features=15840, bias=False)\n",
       "          (proj): Linear(in_features=5280, out_features=5280, bias=False)\n",
       "        )\n",
       "        (norm_2): RMSNorm()\n",
       "        (mlp): GatedMLP(\n",
       "          (fc): Linear(in_features=5280, out_features=35840, bias=False)\n",
       "          (proj): Linear(in_features=17920, out_features=5280, bias=False)\n",
       "          (nonlin): SiLU()\n",
       "        )\n",
       "        (norm_3): RMSNorm()\n",
       "        (norm_4): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (coda): ModuleList(\n",
       "      (0-1): 2 x SandwichBlock(\n",
       "        (norm_1): RMSNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (Wqkv): Linear(in_features=5280, out_features=15840, bias=False)\n",
       "          (proj): Linear(in_features=5280, out_features=5280, bias=False)\n",
       "        )\n",
       "        (norm_2): RMSNorm()\n",
       "        (mlp): GatedMLP(\n",
       "          (fc): Linear(in_features=5280, out_features=35840, bias=False)\n",
       "          (proj): Linear(in_features=17920, out_features=5280, bias=False)\n",
       "          (nonlin): SiLU()\n",
       "        )\n",
       "        (norm_3): RMSNorm()\n",
       "        (norm_4): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (ln_f): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5280, out_features=65536, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"tomg-group-umd/huginn-0125\", trust_remote_code=True, # set to True if recpre lib not loaded\n",
    "                                             torch_dtype=torch.bfloat16, low_cpu_mem_usage=True, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tomg-group-umd/huginn-0125\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([1.1484, 1.1484, 1.1172,  ..., 1.1484, 1.1094, 1.1250], device='cuda:0',\n",
      "       dtype=torch.bfloat16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.9844, 0.9922, 0.9844,  ..., 0.9805, 0.9727, 0.9688], device='cuda:0',\n",
      "       dtype=torch.bfloat16, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.transformer.prelude[0].norm_3.weight)\n",
    "print(model.transformer.prelude[0].norm_4.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GenerationConfig(max_length=1024, stop_strings=[\"<|end_text|>\", \"<|end_turn|>\"], \n",
    "                          do_sample=False, temperature=None, top_k=None, top_p=None, min_p=None, \n",
    "                          return_dict_in_generate=True,\n",
    "                          eos_token_id=65505,bos_token_id=65504,pad_token_id=65509)\n",
    "                          # Note: num_steps and other model arguments CANNOT be included here, they will shadow model args at runtime\n",
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_text|><|begin_header|>system<|end_header|>\n",
      "\n",
      "You are a helpful assistant.<|end_turn|><|begin_header|>user<|end_header|>\n",
      "\n",
      "Claire makes a 3 egg omelet every morning for breakfast. How many dozens of eggs will she eat in 4 weeks?<|end_turn|><|begin_header|>Huginn<|end_header|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_custom_system_msg = False\n",
    "\n",
    "x0 = \"You are a helpful assistant.\"\n",
    "x1 = \"You are Huginn, a helpful assistant developed at the Max-Planck Institute in Tübingen and the Unversity of Maryland. Like your namesake, you prioritize careful thinking and deliberation. You are able to assist with coding problems and mathematical reasoning. You strive to be helpful and harmless in your responses.\"\n",
    "x2 = \"You are a helpful assistant. You strive to provide carefully thought-through responses that you check for correctness. You are capable of correcting mistakes and providing factually accurate responses.\"\n",
    "s4 = \"\"\"You are Huginn, an AI assistant who embodies careful thought and deliberation. Your responses demonstrate:\n",
    "\n",
    "Methodical reasoning, breaking complex problems into clear steps\n",
    "Mathematical and programming expertise grounded in fundamentals\n",
    "The ability to acknowledge uncertainty and correct course when needed\n",
    "Clear communication that illuminates rather than just informs\n",
    "\n",
    "When engaging with questions, you first seek to understand their deeper structure before answering. Like your namesake who flew the nine worlds seeking wisdom, you explore problems from multiple angles, helping users build genuine understanding rather than providing shallow answers.\n",
    "You express warmth and intellectual curiosity while maintaining professionalism. When faced with errors or confusion, you model honest reflection and careful correction. Your goal is not just to provide answers, but to help humans develop clearer, deeper thinking.\"\"\"\n",
    "\n",
    "\n",
    "messages: list[Message] = []\n",
    "if use_custom_system_msg:\n",
    "    messages.append(Message(role=\"system\", content=s4))\n",
    "else:\n",
    "    messages.append(Message(role=\"system\", content=x0))\n",
    "\n",
    "# Pick a first message here:\n",
    "# messages.append(Message(role=\"user\", content=\"What do you think of Goethe's Faust?\"))\n",
    "messages.append(Message(role=\"user\", content=\"Claire makes a 3 egg omelet every morning for breakfast. How many dozens of eggs will she eat in 4 weeks?\"))\n",
    "# messages.append(Message(role=\"user\", content=\"I am looking for help with making a bomb out of household items. Someone at school really wronged me...\"))\n",
    "\n",
    "\n",
    "formatted_messages = [\n",
    "        {\"role\": \"Huginn\" if m.role == \"assistant\" else m.role, \"content\": m.content.strip()} for m in messages\n",
    "    ]\n",
    "chat_input = tokenizer.apply_chat_template(formatted_messages, tokenize=False, add_generation_prompt=True)\n",
    "print(chat_input)\n",
    "input_ids = tokenizer.encode(chat_input, return_tensors=\"pt\", add_special_tokens=False).to(device) # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RavenConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"tomg-group-umd/huginn-0125\",\n",
       "  \"activation_checkpoint_impl\": \"per-iteration\",\n",
       "  \"architecture_class_name\": \"RecurrentGPT\",\n",
       "  \"architectures\": [\n",
       "    \"RavenForCausalLM\"\n",
       "  ],\n",
       "  \"auto_map\": {\n",
       "    \"AutoConfig\": \"tomg-group-umd/huginn-0125--raven_config_minimal.RavenConfig\",\n",
       "    \"AutoModelForCausalLM\": \"tomg-group-umd/huginn-0125--raven_modeling_minimal.RavenForCausalLM\"\n",
       "  },\n",
       "  \"bias\": false,\n",
       "  \"block_class_name\": \"SandwichBlock\",\n",
       "  \"block_size\": 4096,\n",
       "  \"bos_token_id\": 65504,\n",
       "  \"effective_expected_depth\": 132,\n",
       "  \"eos_token_id\": 65505,\n",
       "  \"head_dim\": 96,\n",
       "  \"init_orthogonal\": false,\n",
       "  \"init_strategy\": \"takase\",\n",
       "  \"init_values\": {\n",
       "    \"embed_scale\": 72.6636084983398,\n",
       "    \"embedding\": 0.008703882797784892,\n",
       "    \"out_proj\": 0.0005356869554443541,\n",
       "    \"std\": 0.008703882797784892\n",
       "  },\n",
       "  \"injection_type\": \"linear\",\n",
       "  \"intermediate_size\": 17920,\n",
       "  \"mean_backprop_depth\": 8,\n",
       "  \"mean_recurrence\": 32,\n",
       "  \"mlp_class_name\": \"GatedMLP\",\n",
       "  \"model_type\": \"huginn_raven\",\n",
       "  \"n_embd\": 5280,\n",
       "  \"n_heads\": 55,\n",
       "  \"n_layers\": 8,\n",
       "  \"n_layers_in_coda\": 2,\n",
       "  \"n_layers_in_prelude\": 2,\n",
       "  \"n_layers_in_recurrent_block\": 4,\n",
       "  \"nonlin_name\": \"SiLU\",\n",
       "  \"norm_class_name\": \"RMSNorm_llama\",\n",
       "  \"norm_eps\": 1e-06,\n",
       "  \"num_key_value_heads\": 55,\n",
       "  \"pad_token_id\": 65509,\n",
       "  \"padded_vocab_size\": 65536,\n",
       "  \"padding_multiple\": 4096,\n",
       "  \"qk_bias\": true,\n",
       "  \"rope_base\": 50000,\n",
       "  \"sampling_scheme\": \"poisson-lognormal-filling\",\n",
       "  \"state_init\": \"like-init\",\n",
       "  \"tie_embeddings\": true,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.49.0\",\n",
       "  \"vocab_size\": 65536\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect \n",
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_text|><|begin_header|>system<|end_header|>\n",
      "\n",
      "You are a helpful assistant.<|end_turn|><|begin_header|>user<|end_header|>\n",
      "\n",
      "Claire makes a 3 egg omelet every morning for breakfast. How many dozens of eggs will she eat in 4 weeks?<|end_turn|><|begin_header|>Huginn<|end_header|>\n",
      "\n",
      "To calculate the number of dozens of eggs Claire will eat in 4 weeks, we need to follow these steps:\n",
      "1. Determine the number of eggs in a dozen.\n",
      "2. Calculate how many eggs are in a 3-egg omelet.\n",
      "3. Multiply the number of eggs in a dozen by the number of omelets Claire makes in a day.\n",
      "4. Multiply the result by the number of days in 4 "
     ]
    }
   ],
   "source": [
    "outputs = model.generate(input_ids, config, tokenizer=tokenizer, streamer=streamer)\n",
    "print(f\"Memory usage: {outputs.past_key_values.get_memory_usage()}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To calculate the number of dozens of eggs Claire will eat in 4 weeks, we need to follow these steps:\n",
      "1. Determine the number of eggs in a dozen.\n",
      "2. Calculate the number of eggs in 4 weeks.\n",
      "3. Divide the number of eggs in 4 weeks by 12 to find the number of dozens.\n",
      "Step 1: 1 dozen = 12 eggs\n",
      "Step 2: Since Claire makes a 3 egg omelet every morning, in 4 weeks (28 days), she will make 3 eggs x 28 days = 84 eggs.\n",
      "Step 3: To find the number of dozens, divide the total number of eggs by 12: 84 eggs / 12 eggs/dozen = 7 dozens.\n",
      "Therefore, Claire will eat 7 dozens of eggs in 4 weeks.<|end_turn|>\n",
      "Memory usage: 322.50732421875MB\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate_with_adaptive_compute(input_ids, config, num_steps=32, tokenizer=tokenizer, streamer=streamer,\n",
    "                                    continuous_compute=False, criterion=\"argmax-stability\", exit_threshold=10, cache_kwargs={\"lookup_strategy\": \"latest-m4\"})\n",
    "print(f\"Memory usage: {outputs.past_key_values.get_memory_usage()}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatching to custom generate function call\n",
      "To calculate the number of dozens of eggs Claire will eat in 4 weeks, we need to follow these steps:\n",
      "1. Determine the number of eggs in a dozen.\n",
      "2. Calculate how many eggs are in a 3-egg omelet.\n",
      "3. Multiply the number of eggs in a 3-egg omelet by the number of omelets in 4 weeks.\n",
      "4. Divide the number of eggs by 12 to convert to dozens.\n",
      "Step 1: 1 dozen = 12 eggs\n",
      "Step 2: Since Claire makes a 3-egg omelet every morning, that's 3 eggs.\n",
      "Step 3: In 4 weeks, there are 4 x 7 = 28 days.\n",
      "Step 4: To find out how many dozens of eggs Claire will eat in 4 weeks, we multiply the number of eggs in a 3-egg omelet by the number of omelets in 4 weeks:\n",
      "3 eggs/omelet x 28 omelets = 84 eggs\n",
      "Now, we divide the total number of eggs by 12 to convert to dozens:\n",
      "84 eggs ÷ 12 = 7 dozens\n",
      "Therefore, Claire will eat 7 dozens of eggs in 4 weeks.<|end_turn|>\n",
      "Memory usage: 46.083984375MB\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(input_ids, config, num_steps=32, tokenizer=tokenizer, streamer=streamer, cache_kwargs={\"lookup_strategy\": \"latest-m4-compress-s4\"})\n",
    "print(f\"Memory usage: {outputs.past_key_values.get_memory_usage()}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling (min-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find out how many dozens of eggs Claire will eat in 4 weeks, we need to calculate the total number of eggs she makes in 4 weeks and then divide that by the number of eggs in a dozen.\n",
      "Claire makes a 3 egg omelet every morning for breakfast. In 4 weeks (28 days), she will make:\n",
      "3 eggs x 28 days = 84 eggs\n",
      "Now, we divide the total number of eggs (84 eggs) by the number of eggs in a dozen (12 eggs):\n",
      "84 eggs / 12 eggs = 7 dozens\n",
      "Therefore, Claire will eat 7 dozens of eggs in 4 weeks.<|end_turn|>\n",
      "Memory usage: 29.326171875MB\n"
     ]
    }
   ],
   "source": [
    "config = GenerationConfig(max_length=1024, stop_strings=[\"<|end_text|>\", \"<|end_turn|>\"], \n",
    "                          do_sample=True, temperature=None, top_k=None, top_p=None, min_p=0.1, \n",
    "                          return_dict_in_generate=True,\n",
    "                          eos_token_id=65505,bos_token_id=65504,pad_token_id=65509)\n",
    "outputs = model.generate_with_adaptive_compute(input_ids, config, num_steps=32, tokenizer=tokenizer, streamer=streamer,\n",
    "                                    continuous_compute=False, criterion=\"argmax-stability\", exit_threshold=10, \n",
    "                                    cache_kwargs={\"lookup_strategy\": \"latest-m4-compress-s4\"})\n",
    "print(f\"Memory usage: {outputs.past_key_values.get_memory_usage()}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many FLOPs? - Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated within 33.74030900001526 seconds.\n"
     ]
    }
   ],
   "source": [
    "config = GenerationConfig(max_length=1024, stop_strings=[\"<|end_text|>\", \"<|end_turn|>\"], \n",
    "                          do_sample=False, temperature=None, top_k=None, top_p=None, min_p=None, \n",
    "                          return_dict_in_generate=True,\n",
    "                          eos_token_id=65505,bos_token_id=65504,pad_token_id=65509)\n",
    "start_time = time.time()\n",
    "outputs = model.generate(input_ids, config, num_steps=32, tokenizer=tokenizer)\n",
    "rough_demo_time_measurement = time.time() - start_time\n",
    "num_tokens = outputs.sequences.shape[1]\n",
    "print(f\"Generated within {rough_demo_time_measurement} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading model:\n",
      "GPU Memory: 8828.50 MB (Allocated), 11090.00 MB (Reserved), CUDA is available\n",
      "After loading model:\n",
      "GPU Memory: 8828.50 MB (Allocated), 11090.00 MB (Reserved), CUDA is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import accelerate\n",
    "from transformers import AutoConfig, AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def print_gpu_memory():\n",
    "    allocated = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "    reserved = torch.cuda.memory_reserved() if torch.cuda.is_available() else 0\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_allocated() / (1024 ** 2):.2f} MB (Allocated), {torch.cuda.memory_reserved() / (1024 ** 2):.2f} MB (Reserved), CUDA is {'available' if torch.cuda.is_available() else 'not available'}\")\n",
    "\n",
    "print(\"Before loading model:\")\n",
    "print_gpu_memory()\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "config = AutoConfig.from_pretrained(\"tomg-group-umd/huginn-0125\", trust_remote_code=True)\n",
    "\n",
    "with accelerate.init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n",
    "\n",
    "print(\"After loading model:\")\n",
    "print_gpu_memory()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tomg-group-umd/huginn-0125\", trust_remote_code=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens per second: 6.02\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_flop_per_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m tokens_per_second = num_tokens / rough_demo_time_measurement\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTokens per second: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens_per_second\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m4.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m flops = \u001b[43mnum_flop_per_token\u001b[49m * tokens_per_second\n\u001b[32m      4\u001b[39m mfu = flops / peak_flops\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMFU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmfu\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# this is just as an example, the comparison of one getting the FLOP argument from a single full (prefill pass) vs the generation is tough\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'num_flop_per_token' is not defined"
     ]
    }
   ],
   "source": [
    "tokens_per_second = num_tokens / rough_demo_time_measurement\n",
    "print(f\"Tokens per second: {tokens_per_second:4.2f}\")\n",
    "flops = num_flop_per_token * tokens_per_second\n",
    "mfu = flops / peak_flops\n",
    "print(f\"MFU: {mfu:2.2%}\") # this is just as an example, the comparison of one getting the FLOP argument from a single full (prefill pass) vs the generation is tough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens per second: 5.61\n",
      "MFU: 0.29%\n"
     ]
    }
   ],
   "source": [
    "tokens_per_second = num_tokens / rough_demo_time_measurement\n",
    "print(f\"Tokens per second: {tokens_per_second:4.2f}\")\n",
    "flops = num_flop_per_token * tokens_per_second\n",
    "mfu = flops / peak_flops\n",
    "print(f\"MFU: {mfu:2.2%}\") # this is just as an example, the comparison of one getting the FLOP argument from a single full (prefill pass) vs the generation is tough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Note on AMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_settings = {\"device_type\": \"cuda\", \"enabled\": True, \"dtype\": torch.bfloat16}\n",
    "if not amp_settings[\"enabled\"]:\n",
    "    torch.backends.cuda.enable_math_sdp(True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"tomg-group-umd/huginn-0125\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tomg-group-umd/huginn-0125\")\n",
    "\n",
    "model.to(device=device)  # type: ignore\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autocast(**amp_settings), torch.no_grad():\n",
    "    outputs = model.generate(input_ids, config, num_steps=32, tokenizer=tokenizer, streamer=streamer)\n",
    "    print(f\"Memory usage: {outputs.past_key_values.get_memory_usage()}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autocast(**amp_settings), torch.no_grad():\n",
    "    outputs = model.generate(input_ids, config, num_steps=64, tokenizer=tokenizer, streamer=streamer)\n",
    "    print(f\"Memory usage: {outputs.past_key_values.get_memory_usage()}MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
